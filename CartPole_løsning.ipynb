{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from collections import deque\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opprett en simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_actions,\n",
    "        hidden_layers=[64],\n",
    "        bias=True,\n",
    "        learning_rate=0.001,\n",
    "        experience_capacity=1000\n",
    "    ):\n",
    "        torch.manual_seed(1234)\n",
    "        self.num_features = num_features\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.q_net = self.create_model(hidden_layers, bias)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.q_net.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0\n",
    "        )\n",
    "        \n",
    "        self.experiences = deque(maxlen=experience_capacity)\n",
    "        \n",
    "        self.sync_counter = 0\n",
    "        \n",
    "    def create_model(self, hidden_layers, bias=True):\n",
    "        layer_dims = [self.num_features] + [\n",
    "            layer for layer in hidden_layers if layer > 0\n",
    "        ] + [self.num_actions]\n",
    "        layers = []\n",
    "        for index in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[index], layer_dims[index + 1], bias=bias))\n",
    "            layers.append(nn.Identity() if index == len(layer_dims) - 2 else nn.Tanh())\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def get_action(self, state, epsilon=0):\n",
    "        with torch.no_grad():\n",
    "            Qp = self.q_net(state)\n",
    "        Q, A = torch.max(Qp, axis=0)\n",
    "        A = A if torch.rand(1, ).item() > epsilon else torch.randint(0, self.num_actions, (1,))\n",
    "        return A\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        self.experiences.append(experience)\n",
    "        \n",
    "    def get_experience(self, batch_size):\n",
    "        if len(self.experiences) < batch_size:\n",
    "            batch_size = len(self.experiences)\n",
    "        sample = random.sample(self.experiences, batch_size)\n",
    "        states = torch.stack([exp[0] for exp in sample]).float()\n",
    "        actions = torch.tensor([exp[1] for exp in sample]).float()\n",
    "        rewards = torch.tensor([exp[2] for exp in sample]).float()\n",
    "        next_states = torch.tensor([exp[3] for exp in sample]).float()\n",
    "        return states, actions, rewards, next_states\n",
    "\n",
    "    def get_q_next(self, state):\n",
    "        with torch.no_grad():\n",
    "            qp = self.target_net(state)\n",
    "        q, _ = torch.max(qp, axis=1)\n",
    "        return q\n",
    "    \n",
    "    def fit(self, batch_size, gamma=0.95):\n",
    "        states, actions, rewards, next_states = self.get_experience(batch_size)\n",
    "        \n",
    "        if self.sync_counter == 1:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            self.target_net.eval()\n",
    "            self.sync_counter = 0\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        pred_return, _ = torch.max(q_pred, axis=1)\n",
    "\n",
    "        # get target return using target network\n",
    "        q_next = self.get_q_next(next_states)\n",
    "        target_return = rewards + gamma * q_next\n",
    "\n",
    "        loss = self.loss_func(pred_return, target_return)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_value_(self.q_net.parameters(), clip_value=0.75)\n",
    "        #nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=2.0, norm_type=2)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.sync_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "epsilon = 1\n",
    "epsilon_decay = 1 / 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forskjellige learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Learning rate = 0.01\n",
      "Episode 0: 18.0\n",
      "Episode 500: 27.6\n",
      "Episode 1000: 31.16\n",
      "Episode 1500: 39.5\n",
      "Episode 2000: 47.99\n",
      "Episode 2500: 72.17\n",
      "Episode 3000: 81.39\n",
      "Episode 3500: 112.29\n",
      "Episode 4000: 163.73\n",
      "Episode 4500: 188.03\n",
      "Finished at episode 4856: 197.91\n",
      "\n",
      "\n",
      "Learning rate = 0.001\n",
      "Episode 0: 18.0\n",
      "Episode 500: 24.87\n",
      "Episode 1000: 33.53\n",
      "Episode 1500: 45.52\n",
      "Episode 2000: 58.96\n",
      "Episode 2500: 81.57\n",
      "Episode 3000: 117.48\n",
      "Episode 3500: 140.93\n",
      "Episode 4000: 185.26\n",
      "Finished at episode 4135: 197.62\n",
      "\n",
      "\n",
      "Learning rate = 0.00075\n",
      "Episode 0: 17.0\n",
      "Episode 500: 25.55\n",
      "Episode 1000: 32.7\n",
      "Episode 1500: 39.42\n",
      "Episode 2000: 58.27\n",
      "Episode 2500: 69.93\n",
      "Episode 3000: 88.55\n",
      "Episode 3500: 100.82\n",
      "Episode 4000: 112.07\n",
      "Episode 4500: 143.94\n",
      "Finished at episode 4794: 197.61\n",
      "\n",
      "\n",
      "Learning rate = 0.0005\n",
      "Episode 0: 20.0\n",
      "Episode 500: 24.85\n",
      "Episode 1000: 33.21\n",
      "Episode 1500: 42.65\n",
      "Episode 2000: 53.33\n",
      "Episode 2500: 63.36\n",
      "Episode 3000: 81.68\n",
      "Episode 3500: 96.64\n",
      "Episode 4000: 109.89\n",
      "Episode 4500: 147.34\n",
      "Finished at episode 4968: 197.62\n",
      "\n",
      "\n",
      "Learning rate = 0.00025\n",
      "Episode 0: 15.0\n",
      "Episode 500: 24.86\n",
      "Episode 1000: 32.38\n",
      "Episode 1500: 45.18\n",
      "Episode 2000: 49.71\n",
      "Episode 2500: 56.58\n",
      "Episode 3000: 72.8\n",
      "Episode 3500: 80.62\n",
      "Episode 4000: 96.33\n",
      "Episode 4500: 134.72\n",
      "Episode 5000: 149.81\n",
      "Episode 5500: 158.13\n",
      "Episode 6000: 165.26\n",
      "Episode 6500: 158.14\n",
      "Episode 7000: 153.24\n",
      "Episode 7500: 164.56\n",
      "Finished at episode 7500: 164.56\n",
      "\n",
      "\n",
      "Learning rate = 0.0001\n",
      "Episode 0: 15.0\n",
      "Episode 500: 25.5\n",
      "Episode 1000: 32.33\n",
      "Episode 1500: 39.57\n",
      "Episode 2000: 49.61\n",
      "Episode 2500: 50.93\n",
      "Episode 3000: 64.84\n",
      "Episode 3500: 68.45\n",
      "Episode 4000: 83.74\n",
      "Episode 4500: 105.76\n",
      "Episode 5000: 117.95\n",
      "Episode 5500: 111.21\n",
      "Episode 6000: 114.1\n",
      "Episode 6500: 105.37\n",
      "Episode 7000: 106.09\n",
      "Episode 7500: 89.55\n",
      "Finished at episode 7500: 89.55\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [0.01, 0.001, 0.00075, 0.0005, 0.00025, 0.0001]:\n",
    "    agent = Agent(\n",
    "        num_features = env.observation_space.shape[0],\n",
    "        num_actions = env.action_space.n,\n",
    "        hidden_layers = [64],\n",
    "        bias = False,\n",
    "        learning_rate = learning_rate,\n",
    "        experience_capacity = 10000\n",
    "    )\n",
    "\n",
    "    epsilon = 1\n",
    "    episode_durations = []\n",
    "    print(f'\\n\\nLearning rate = {learning_rate}')\n",
    "\n",
    "    for episode in range(7501):\n",
    "        state, done = env.reset(), False\n",
    "\n",
    "        for timestep in count():\n",
    "            state = torch.tensor(state)\n",
    "\n",
    "            action = agent.get_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            agent.add_experience([state, action.item(), reward, next_state])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                #for _ in range(2 + round(timestep / 50)):\n",
    "                agent.fit(batch_size, gamma)\n",
    "                episode_durations.append(timestep)\n",
    "                break\n",
    "\n",
    "\n",
    "        if epsilon > 0.05 :\n",
    "            epsilon -= epsilon_decay\n",
    "\n",
    "        avg = np.mean(episode_durations[-100:])\n",
    "\n",
    "        if episode % 500 == 0:\n",
    "            print(f'Episode {episode}: {avg}')\n",
    "\n",
    "        if avg > 197.5:\n",
    "            break\n",
    "\n",
    "    print(f'Finished at episode {episode}: {avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se pÃ¥ utviklingen til en agent mens den trener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/drf1dc212g3dgfhqpz6vpxww0000gn/T/ipykernel_99961/426021851.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  next_states = torch.tensor([exp[3] for exp in sample]).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 08:57:38.656 Python[99961:6490018] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/0l/drf1dc212g3dgfhqpz6vpxww0000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500: 24.87\n",
      "Episode 1000: 34.25\n",
      "Episode 1500: 44.75\n",
      "Episode 2000: 60.91\n",
      "Episode 2500: 77.14\n",
      "Episode 3000: 119.82\n",
      "Episode 3500: 175.68\n",
      "Episode 4000: 322.89\n",
      "Episode 4500: 416.83\n",
      "Episode 5000: 494.82\n",
      "Episode 5500: 486.02\n",
      "Episode 6000: 492.3\n",
      "Episode 6500: 493.15\n",
      "Episode 7000: 490.05\n",
      "Episode 7500: 492.96\n",
      "Episode 8000: 494.63\n",
      "Episode 8500: 497.23\n",
      "Episode 9000: 496.75\n",
      "Episode 9500: 492.09\n",
      "Episode 10000: 495.35\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    num_features = env.observation_space.shape[0],\n",
    "    num_actions = env.action_space.n,\n",
    "    hidden_layers = [64],\n",
    "    bias = False,\n",
    "    learning_rate = 0.01,\n",
    "    experience_capacity = 10000\n",
    ")\n",
    "\n",
    "epsilon = 1\n",
    "episode_durations = []\n",
    "\n",
    "for episode in range(10001):\n",
    "    state, done = env.reset(), False\n",
    "\n",
    "    for timestep in count():\n",
    "        state = torch.tensor(state)\n",
    "\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        agent.add_experience([state, action.item(), reward, next_state])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            #for _ in range(2 + round(timestep / 50)):\n",
    "            agent.fit(batch_size, gamma)\n",
    "            episode_durations.append(timestep)\n",
    "            break\n",
    "\n",
    "\n",
    "    if epsilon > 0.05 :\n",
    "        epsilon -= epsilon_decay\n",
    "\n",
    "    avg = np.mean(episode_durations[-100:])\n",
    "\n",
    "    if episode % 500 == 0:\n",
    "        print(f'Episode {episode}: {avg}')\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            state = torch.tensor(state)\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 08:51:58.394 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.396 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.398 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.399 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.401 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.403 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.405 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.406 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.408 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.410 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.412 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.413 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.415 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.417 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.419 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.420 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.422 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.424 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.426 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.427 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.429 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.431 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.432 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.434 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.435 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.439 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.440 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.442 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.444 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.445 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.447 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.449 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.450 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.452 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.454 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.456 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.458 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.460 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.462 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.464 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.466 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.469 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.470 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.472 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.475 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.477 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.479 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.481 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.483 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.485 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n",
      "2022-02-11 08:51:58.487 Python[95760:6219866] _mthid_copyDeviceInfo(288230376672660929) failed\n"
     ]
    }
   ],
   "source": [
    "env._max_episode_steps = 5000\n",
    "\n",
    "while True:\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        state = torch.tensor(state)\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
