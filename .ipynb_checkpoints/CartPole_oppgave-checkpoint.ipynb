{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppgave!\n",
    "\n",
    "Her skal dere selv trene opp en agent til å utføre en oppgave.\n",
    "Oppgaven er å lage en agent som kan balansere en pinne på en liten tralle. Dette problemet er kjent som **CartPole**.\n",
    "\n",
    "Du kan lese mer om problemet på følgende link. Legg særlig merke til hva slags outputs du får fra denne (states), og hva slags inputs du kan gi til den (actions).\n",
    "\n",
    "[CartPole v0 på github](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "\n",
    "![Cartpole](https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/poster.jpg)\n",
    "\n",
    "\n",
    "### Utvidelse av oppgaven\n",
    "\n",
    "Du kommer sikkert til å oppleve at agent ikke holder trallen på midten av skjermen. Den bruker gjerne litt tid på det, men sklir til slutt ut til siden.\n",
    "\n",
    "Med en enkel liten endring kan du trene agenten til å holde trallen på midten av skjermen. Kan du finne ut hvordan?\n",
    "\n",
    "Hint: Tenk på hvilken reward du gir agenten i \"fit\"-funksjonen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from collections import deque\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opprett en simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_actions,\n",
    "        hidden_layers=[64],\n",
    "        bias=True,\n",
    "        learning_rate=0.001,\n",
    "        experience_capacity=1000\n",
    "    ):\n",
    "        torch.manual_seed(1234)\n",
    "        self.num_features = num_features\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.q_net = self.create_model(hidden_layers, bias)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.q_net.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0\n",
    "        )\n",
    "        \n",
    "        self.experiences = deque(maxlen=experience_capacity)\n",
    "        \n",
    "        self.sync_counter = 0\n",
    "        \n",
    "    def create_model(self, hidden_layers, bias=True):\n",
    "        layer_dims = [self.num_features] + [\n",
    "            layer for layer in hidden_layers if layer > 0\n",
    "        ] + [self.num_actions]\n",
    "        layers = []\n",
    "        for index in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[index], layer_dims[index + 1], bias=bias))\n",
    "            layers.append(nn.Identity() if index == len(layer_dims) - 2 else nn.Tanh())\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def get_action(self, state, epsilon=0):\n",
    "        with torch.no_grad():\n",
    "            Qp = self.q_net(state)\n",
    "        Q, A = torch.max(Qp, axis=0)\n",
    "        A = A if torch.rand(1, ).item() > epsilon else torch.randint(0, self.num_actions, (1,))\n",
    "        return A\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        self.experiences.append(experience)\n",
    "        \n",
    "    def get_experience(self, batch_size):\n",
    "        if len(self.experiences) < batch_size:\n",
    "            batch_size = len(self.experiences)\n",
    "        sample = random.sample(self.experiences, batch_size)\n",
    "        states = torch.stack([exp[0] for exp in sample]).float()\n",
    "        actions = torch.tensor([exp[1] for exp in sample]).float()\n",
    "        rewards = torch.tensor([exp[2] for exp in sample]).float()\n",
    "        next_states = torch.tensor([exp[3] for exp in sample]).float()\n",
    "        return states, actions, rewards, next_states\n",
    "\n",
    "    def get_q_next(self, state):\n",
    "        with torch.no_grad():\n",
    "            qp = self.target_net(state)\n",
    "        q, _ = torch.max(qp, axis=1)\n",
    "        return q\n",
    "    \n",
    "    # Trening!\n",
    "    def fit(self, batch_size, gamma=0.95):\n",
    "        states, actions, rewards, next_states = self.get_experience(batch_size)\n",
    "        \n",
    "        if self.sync_counter == 1:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            self.target_net.eval()\n",
    "            self.sync_counter = 0\n",
    "        \n",
    "        # Predicted return\n",
    "        q_pred = self.q_net(states)\n",
    "        pred_return, _ = torch.max(q_pred, axis=1)\n",
    "\n",
    "        # Target return\n",
    "        q_next = self.get_q_next(next_states)\n",
    "        target_return = rewards + gamma * q_next\n",
    "\n",
    "        loss = self.loss_func(pred_return, target_return)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_value_(self.q_net.parameters(), clip_value=0.75)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.sync_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trene en agent\n",
    "\n",
    "Husk at:\n",
    "\n",
    "- Learning rate styrer hvor fort agenten skal lære fra nye opplevelser, men dermed også hvor fort den glemmer gamle.\n",
    "- Hidden layers er en array der du kan bestemme hvor mange nevroner du vil ha i hver layer mellom input og output.\n",
    "- Epsilon er exploration rate. I starten av treningen er det lulrt at agenten utforsker miljøet mye, men denne bør minke etterhvert som agenten blir flinkere. Bruk epsilon decay til dette.\n",
    "\n",
    "Med disse parameterne vil ikke nødvendigvis agenten klare å lære seg oppgaven. Lek litt med dem! Du kan gjerne loope gjennom noen parametre for å finne frem til noe som fungerer.\n",
    "\n",
    "Fungerende kode kan finnes i \"løsning\"-notebooken, men ikke se på denne før du har prøvd litt selv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "epsilon = 1\n",
    "epsilon_decay = 1 / 5000\n",
    "\n",
    "agent = Agent(\n",
    "    num_features = env.observation_space.shape[0],\n",
    "    num_actions = env.action_space.n,\n",
    "    hidden_layers = [16, 16],\n",
    "    bias = True,\n",
    "    learning_rate = 0.001,\n",
    "    experience_capacity = 10000\n",
    ")\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "for episode in range(7501):\n",
    "    state, done = env.reset(), False\n",
    "\n",
    "    for timestep in count():\n",
    "        state = torch.tensor(state)\n",
    "\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        agent.add_experience([state, action.item(), reward, next_state])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            #for _ in range(2 + round(timestep / 50)):\n",
    "            agent.fit(batch_size, gamma)\n",
    "            episode_durations.append(timestep)\n",
    "            break\n",
    "\n",
    "\n",
    "    if epsilon > 0.05 :\n",
    "        epsilon -= epsilon_decay\n",
    "\n",
    "    avg = np.mean(episode_durations[-100:])\n",
    "\n",
    "    if episode % 500 == 0:\n",
    "        print(f'Episode {episode}: {avg}')\n",
    "\n",
    "    if avg > 197.5:\n",
    "        break\n",
    "\n",
    "print(f'Finished at episode {episode}: {avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se på utviklingen til en agent mens den trener\n",
    "\n",
    "Dette er litt gøy!\n",
    "\n",
    "Her får du en visualisering av at agenten forsøker å balansere en pinne. For hver 500 iterasjon med trening vil det spilles av én episode på skjermen din.\n",
    "\n",
    "Denne koden bruker lengre tid på å kjøre enn koden ovenfor, så finn gjerne frem til noen parametre som fungerer før du tester de parametrene her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "epsilon = 1\n",
    "epsilon_decay = 1 / 5000\n",
    "\n",
    "agent = Agent(\n",
    "    num_features = env.observation_space.shape[0],\n",
    "    num_actions = env.action_space.n,\n",
    "    hidden_layers = [16, 16],\n",
    "    bias = False,\n",
    "    learning_rate = 0.001,\n",
    "    experience_capacity = 10000\n",
    ")\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "for episode in range(10001):\n",
    "    state, done = env.reset(), False\n",
    "\n",
    "    for timestep in count():\n",
    "        state = torch.tensor(state)\n",
    "\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        agent.add_experience([state, action.item(), reward, next_state])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            #for _ in range(2 + round(timestep / 50)):\n",
    "            agent.fit(batch_size, gamma)\n",
    "            episode_durations.append(timestep)\n",
    "            break\n",
    "\n",
    "\n",
    "    if epsilon > 0.05 :\n",
    "        epsilon -= epsilon_decay\n",
    "\n",
    "    avg = np.mean(episode_durations[-100:])\n",
    "\n",
    "    if episode % 500 == 0:\n",
    "        print(f'Episode {episode}: {avg}')\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            state = torch.tensor(state)\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            env.render(\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balansering til evig tid\n",
    "\n",
    "Her sørger vi for at hver episode kan spille av mye lengre, og agenten får prøve å balansere så lenge den klarer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._max_episode_steps = 5000\n",
    "\n",
    "while True:\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        state = torch.tensor(state)\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
